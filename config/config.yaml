defaults:
  - data: mnist_small
  - model: mlp_small
  - _self_

device: [0] # relevant devices are given by list (-1 is cpu) # this might not work on cluster
device_torch: "cuda:0"
seed: 2

epoch:
  start: 0
  end: 50
  frequ_to_save: 1

optim:
  adam:
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
  lr: 4e-4 # note that init_lr is (lr * batch_size / 256) 
  wd: 1e-4 # wd = lambda / N with lambda precision of prior Gaussian, N number of training samples
  bs: 256
  n_workers: 3
  scheduler:
    enable: True
    warmup: 0.1 # warmup phase measured in epoch.end
    decay: 0.5 # start to decay
  
swa: # swa method
  enable: False # should swa be applied
  epoch:
    start: 0.5 # from which epoch should swa be applied
    frequ_to_save: 1 # frequency to store models
    annealing: 0 # number of annealing epochs
  n_save: 20 # Saves best n models
  lr: 4e-2 # swa learning rate

ckpt: # rename state-dict
  name: "epoch=29-step=7050"
  version: 0

hydra:
  job:
    chdir: False  